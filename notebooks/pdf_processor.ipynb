{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os\n",
    "from datetime import datetime\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "log_dir = 'logs'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "log_file = os.path.join(log_dir, f'rag-api-{current_date}.log')\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Literal, List\n",
    "import pathlib\n",
    "from abc import abstractmethod\n",
    "from pydantic import BaseModel, field_validator\n",
    "\n",
    "class DocumentProcessorInterface(BaseModel):   \n",
    "    \"\"\"\n",
    "    Provides abstract interface and shared functionality for document of different types\n",
    "    \"\"\"\n",
    "    file_path: pathlib.Path \n",
    "    \n",
    "    # Runs file_path validation before any processing\n",
    "    @field_validator(\"file_path\")\n",
    "    def validate_file_path(cls, value: Union[str, pathlib.Path]) -> pathlib.Path:\n",
    "        if isinstance(value, str):\n",
    "            return pathlib.Path(value)\n",
    "        return value\n",
    "\n",
    "    @property\n",
    "    def filename(self) -> str:\n",
    "        return self.file_path.name\n",
    "    \n",
    "    @abstractmethod\n",
    "    def cleanup(self) -> None:\n",
    "        \"\"\"Closes documents manually\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def extract_text(self, format: str) -> str:\n",
    "        raise NotImplementedError    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class TextExtractor(Enum):\n",
    "    FITZ = \"fitz\"\n",
    "    OCR = \"ocr\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from PIL import Image\n",
    "\n",
    "class PDFDocumentProcessorInterface(DocumentProcessorInterface):\n",
    "    \"\"\"\n",
    "    Provides abstract interface and shared functionality for PDF Documents Processors\n",
    "    \"\"\"\n",
    "    \n",
    "    fitz_doc: fitz.Document = None\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True  # Skip validation for unsupported types like fitz.Document, fitz.Page\n",
    "\n",
    "    def open_fitz_doc(self) -> None:\n",
    "        if self.fitz_doc is None:\n",
    "            self.fitz_doc =fitz.open(self.file_path)\n",
    "\n",
    "    def cleanup(self) -> None:\n",
    "        if self.fitz_doc:\n",
    "            self.fitz_doc.close()\n",
    "\n",
    "    def get_fitz_page(self, page_number: int) -> fitz.Page:\n",
    "        if not self.fitz_doc:\n",
    "            self.open_fitz_doc()\n",
    "        return self.fitz_doc.load_page(page_number)\n",
    "    \n",
    "    def get_page_image(self, page_number: int) -> Image.Image:\n",
    "        if not self.fitz_doc:\n",
    "            self.open_fitz_doc()\n",
    "        page = self.fitz_doc.load_page(page_number)\n",
    "        pixmap = page.get_pixmap()\n",
    "        image = Image.frombytes(\"RGB\", [pixmap.width, pixmap.height], pixmap.samples)       \n",
    "        return image\n",
    "    \n",
    "    def get_text_extractor(self) -> TextExtractor:\n",
    "        # TODO OCR Implementation\n",
    "        return TextExtractor.FITZ\n",
    "        # return TextExtractor.OCR\n",
    "        \n",
    "    \n",
    "    def extract_text(self, format: Literal['raw', 'md']='raw', pagewise: bool=False) -> Union[List[str], str]:\n",
    "        text_extractor = self.get_text_extractor()\n",
    "        assert text_extractor == TextExtractor.FITZ, \"Only Fitz Text extractor is supported as of now!! Thank you for your patience.\"\n",
    "        \n",
    "        if format=='raw':\n",
    "            self.open_fitz_doc()\n",
    "            if pagewise:\n",
    "                document_text = []\n",
    "                for page in self.fitz_doc.pages():\n",
    "                    page_text = page.get_text()\n",
    "                    document_text.append(page_text)\n",
    "                return document_text                \n",
    "            else:\n",
    "                document_text=\"\"\n",
    "                for page in self.fitz_doc.pages():\n",
    "                    page_text = page.get_text()\n",
    "                    document_text += page_text +\"\\n\\n-<PAGE_BREAK>-\\n\\n\"\n",
    "                return document_text\n",
    "        elif format==\"md\":\n",
    "            import pymupdf4llm\n",
    "            md_text = pymupdf4llm.to_markdown(self.file_path)\n",
    "            return md_text\n",
    "        else:\n",
    "            raise ValueError(\"Only 'raw' and 'md formats are supported as of now!!\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pdf = \"/home/bibekyess/yolo/plain_rag/FastRAG/tests/llama2.pdf\"\n",
    "\n",
    "pdf_processor = PDFDocumentProcessorInterface(file_path=input_pdf)\n",
    "pdf_processor.extract_text(pagewise=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.readers.base import BaseReader\n",
    "from typing import Any, List, Optional, Dict\n",
    "from pathlib import Path\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "\n",
    "class FitzPDFReader(BaseReader):\n",
    "    def __init__(\n",
    "        self, chunk_size: int=512, chunk_stride:int = 256, pagenum: int=-1, \n",
    "        split_documents: bool=True, page_wise: bool=False, overlap: bool=True, \n",
    "        *args: Any, **kwargs: Any\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if chunk_size <= 0: chunk_size=256\n",
    "        if chunk_stride < 0: chunk_stride=chunk_size//2\n",
    "        if chunk_stride > chunk_size: chunk_stride=chunk_size//2\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_stride = chunk_stride\n",
    "        self.pagenum = pagenum\n",
    "        self.split_documents = split_documents\n",
    "        self.page_wise = page_wise\n",
    "        self.overlap = overlap\n",
    "        if not self.overlap:\n",
    "            self.chunk_stride=0\n",
    "\n",
    "\n",
    "    def load_data(\n",
    "        self, file: Union[str, Path], extra_info: Optional[Dict] = {}\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load data and extract document chunks from corresponding file contents.\n",
    "        Please don't modify the arguments passed here as LlamaIndex's SimpleDirectoryReader expects these arguments only.\n",
    "        ALIRAG uses SimpleDirectoryReader to process the files/list of files and directory.\n",
    "\n",
    "        Args:\n",
    "            file (str): A url or file path pointing to the document\n",
    "            extra_info (Optional[Dict]): Additional information that might be needed for loading the data, by default None.\n",
    "                LlamaIndex SimpleDirectoryReader by default provides some basic File metadatas with this parameter\n",
    "        Returns:\n",
    "            List[Document]: List of documents.\n",
    "        \"\"\"\n",
    "\n",
    "        def get_sentence_chunks(text, chunk_size, chunk_stride):\n",
    "            from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                        chunk_size = chunk_size,\n",
    "                        chunk_overlap  = chunk_stride,\n",
    "                        length_function = lambda x: len(x.split()),\n",
    "                        is_separator_regex = False,\n",
    "                    )\n",
    "\n",
    "            texts = text_splitter.create_documents([text])\n",
    "            return [t.page_content for t in texts]\n",
    "        \n",
    "        pdf_processor = PDFDocumentProcessorInterface(file_path=str(file))\n",
    "        page_wise_texts = pdf_processor.extract_text(pagewise=True)\n",
    "\n",
    "        results = []\n",
    "        prev_chunk_preceding_content = None\n",
    "\n",
    "        entire_text = \"\"\n",
    "        document = None\n",
    "        \n",
    "        for pagenum, document_chunks in enumerate(page_wise_texts):\n",
    "\n",
    "            if not self.split_documents:\n",
    "                entire_text += document_chunks.strip() + \"\\n\\n----\\n\\n\" # FIXME '----' represents page splits\n",
    "                continue\n",
    "\n",
    "            if not self.page_wise:\n",
    "                texts = get_sentence_chunks(text=document_chunks, chunk_size=self.chunk_size, chunk_stride=self.chunk_stride)\n",
    "            else:\n",
    "                texts = [document_chunks]  \n",
    "        \n",
    "            for idx, text in enumerate(texts):\n",
    "                if self.pagenum != -1:\n",
    "                    extra_info[\"page_label\"] = self.pagenum # This is being inquired about the specific page\n",
    "                else:\n",
    "                    extra_info[\"page_label\"] = pagenum\n",
    "                document = Document(\n",
    "                    text=text, extra_info=extra_info\n",
    "                )\n",
    "                results.append(document)\n",
    "\n",
    "            if prev_chunk_preceding_content is not None and len(texts) > 0:\n",
    "                current_whole_text = document.text\n",
    "                current_head_text = get_sentence_chunks(text=current_whole_text, chunk_size=self.chunk_size//2, chunk_stride=0)[0]\n",
    "                previous_whole_text = prev_chunk_preceding_content.text\n",
    "                previous_tail_text = get_sentence_chunks(text=previous_whole_text, chunk_size=self.chunk_size//2, chunk_stride=0)[-1]\n",
    "                overlapping_text = previous_tail_text + current_head_text\n",
    "                metadata = prev_chunk_preceding_content.metadata\n",
    "                metadata[\"overlapping\"] = True\n",
    "                overlapping_document = Document(\n",
    "                    text=overlapping_text, extra_info=metadata\n",
    "                )\n",
    "                results.append(overlapping_document)\n",
    "            \n",
    "            if self.overlap and document is not None:\n",
    "                prev_chunk_preceding_content = document\n",
    "        \n",
    "        if not self.split_documents:\n",
    "            results = [Document(\n",
    "                text=entire_text, extra_info=extra_info\n",
    "            )]\n",
    "\n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = FitzPDFReader(overlap=False)\n",
    "documents = pdf_reader.load_data(file=input_pdf)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "llama_reader = pymupdf4llm.LlamaMarkdownReader()\n",
    "llama_docs = llama_reader.load_data(\"/home/bibekyess/yolo/plain_rag/FastRAG/tests/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(llama_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add documents to the vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "\n",
    "qdrant_url = \"http://0.0.0.0:6333\"\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "qdrant_client = QdrantClient(url=qdrant_url, timeout=20)\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=model_name, max_length=512) # FIXME Changel max_length to consider high memory usage with bge-m3\n",
    "\n",
    "\n",
    "def initialize_text_node_from_document(doc, embed_model) -> TextNode:\n",
    "    doc_data: Dict[str, Any] = {\n",
    "        \"id_\": doc.id_,\n",
    "        \"embedding\": embed_model.get_text_embedding(doc.text),\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"excluded_embed_metadata_keys\": doc.excluded_embed_metadata_keys,\n",
    "        \"excluded_llm_metadata_keys\": doc.excluded_llm_metadata_keys,\n",
    "        \"relationships\": doc.relationships,\n",
    "        \"text\": doc.text,\n",
    "        \"mimetype\": doc.mimetype,\n",
    "        \"start_char_idx\": doc.start_char_idx,\n",
    "        \"end_char_idx\": doc.end_char_idx,\n",
    "        \"text_template\": doc.text_template,\n",
    "        \"metadata_template\": doc.metadata_template,\n",
    "        \"metadata_seperator\": doc.metadata_seperator,\n",
    "    }\n",
    "\n",
    "    text_node = TextNode(**doc_data)\n",
    "    return text_node\n",
    "\n",
    "def add_documents_to_index(\n",
    "    documents: List[Document],\n",
    "    index_id: str,\n",
    "):\n",
    "    \"\"\"Upload documents to an index\"\"\"\n",
    "    nodes = [initialize_text_node_from_document(d, embed_model) for d in documents]\n",
    "\n",
    "    # Load the vector store\n",
    "    qdrant_vector_store = QdrantVectorStore(\n",
    "        client=qdrant_client,\n",
    "        collection_name=index_id,\n",
    "    )\n",
    "\n",
    "    # Add nodes to vector index\n",
    "    document_ids = qdrant_vector_store.add(nodes)\n",
    "    return {\"document_ids\": document_ids}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(llama_docs), len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_docs[0].id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_documents_to_index(llama_docs, 'delete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastrag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
