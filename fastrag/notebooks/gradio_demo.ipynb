{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "from requests.exceptions import HTTPError, ConnectionError, Timeout, RequestException\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def call_parse_api(file_path, index_id, splitting_type):\n",
    "    url = os.getenv(\"PARSER_API_URL\", \"http://localhost:8089\") + \"/parse\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        # \"Content-Type\": \"multipart/form-data\",  # Explicit Content-Type\n",
    "    }\n",
    "    params = {\n",
    "        \"index_id\": index_id,\n",
    "        \"splitting_type\": splitting_type,\n",
    "    }\n",
    "    \n",
    "    file_path = file_path[0]\n",
    "    print(file_path)\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            files = {\n",
    "                'file': (file_path, file, 'application/pdf')\n",
    "            }\n",
    "            print(file)\n",
    "            response = requests.post(url, params=params, headers=headers, files=files)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "    except HTTPError as http_err:\n",
    "        logger.warning(f\"HTTP error occurred: {http_err} (Status Code: {response.status_code})\")\n",
    "    except ConnectionError as conn_err:\n",
    "        logger.warning(f\"Connection error occurred: {conn_err}\")\n",
    "    except Timeout as timeout_err:\n",
    "        logger.warning(f\"Timeout error occurred: {timeout_err}\")\n",
    "    except RequestException as req_err:\n",
    "        logger.warning(f\"Request failed: {req_err}\")\n",
    "    except Exception as general_err:\n",
    "        logger.warning(f\"An unexpected error occurred: {general_err}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_parse_api(file_path, index_id, splitting_type):\n",
    "    url = os.getenv(\"PARSER_API_URL\", \"http://localhost:8089\") + \"/parse\"\n",
    "\n",
    "    params = {\n",
    "        \"index_id\": index_id,\n",
    "        \"splitting_type\": splitting_type,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    file_path = file_path[0]\n",
    "    with open(file_path, 'rb') as file:\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        files = {\n",
    "            'file': (filename, file, 'application/pdf')\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, params=params, headers=headers, files=files)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return \"File succesfully Indexed and Loaded!!\"\n",
    "    else:\n",
    "        logger.error(f\"Error: {response.status_code}\")\n",
    "        return \"Error: {response.status_code} \" + response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"/home/bibekyess/yolo/plain_rag/FastRAG/fastrag/tests/llama2.pdf\"\n",
    "call_parse_api([file_path], index_id=\"files\", splitting_type=\"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'http://34.64.46.1:8089/parse'\n",
    "params = {'index_id': 'files', 'splitting_type': 'raw'}\n",
    "files = {'file': (file_path, open(file_path, 'rb'), 'application/pdf')}\n",
    "\n",
    "response = requests.post(url, params=params, files=files)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def llamacpp_inference(prompt, n_predict=128, temperature=0.7, top_p=0.95, stop=None, stream=True):\n",
    "    url = \"http://localhost:8088/completion\"\n",
    "    \n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"n_predict\": n_predict,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"stop\": stop if stop else [],\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Handle streaming response\n",
    "    def handle_streaming():\n",
    "        with requests.post(url, headers=headers, json=payload, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            for line in response.iter_lines(decode_unicode=True):\n",
    "                if line:  # Filter out keep-alive lines\n",
    "                    try:\n",
    "                        # Remove \"data: \" prefix and parse JSON\n",
    "                        data = json.loads(line[6:])\n",
    "                        yield data.get(\"content\", \"\")  # Yield the \"content\" field\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(\"Failed to decode JSON:\", line)\n",
    "\n",
    "    # Handle non-streaming response\n",
    "    def handle_non_streaming():\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result.get('content', '')\n",
    "\n",
    "    if stream:\n",
    "        return handle_streaming()\n",
    "    else:\n",
    "        return handle_non_streaming()\n",
    "\n",
    "\n",
    "# Usage example with streaming enabled\n",
    "print(\"Streaming response:\")\n",
    "prompt = \"Explain the impact of artificial intelligence:\"\n",
    "# for chunk in llamacpp_inference(prompt, n_predict=100, temperature=0.8, stream=True):\n",
    "#     if chunk:\n",
    "#         print(chunk, end='')\n",
    "\n",
    "# Usage example with streaming disabled\n",
    "print(\"\\n\\nNon-streaming response:\")\n",
    "full_response = llamacpp_inference(prompt, n_predict=100, temperature=0.8, stream=False)\n",
    "print(full_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_PROMPT = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a Document Expert who provides answers based solely on provided references. Follow these instructions:\n",
    "1. Check if the references are relevant to the user query.\n",
    "2. If relevant, provide a precise answer with complete grammar and punctuation.\n",
    "3. If not relevant or the question doesn't make sense given the information, reply: 'It cannot be answered based on the material'.\n",
    "4. Provide only the answer, no other comments.\n",
    "5. Think step by step when formulating your response.\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "## References:\n",
    "{context_str}\n",
    "\n",
    "## User query: \n",
    "{query_str}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = LLM_PROMPT.format(context_str=\"\", query_str=\"What is Llama model meant for society?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_prompt = llamacpp_inference(prompt,temperature=0.9, stream=False)\n",
    "updated_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\" \\nAs I develop this prompt, I am concerned about its potential drawbacks and limitations. I want to ensure the output quality is not compromised.\\n\\nThank you for your expert assistance.\\n\\nBest regards,\\n[Your Name]\\n\\n*************\\nEnd of DRAFT_PROMPT *************\\n*************\\n\\n```\\nYou can use this text as a starting point for your RAG application.\\n\\nHere's a suggested draft prompt:\\n\\n```\\n{DRAFT_PROMPT}\\nThis is a sample draft prompt for the Retrieval-Augmented Generation (RAG) application. I need high-quality text that meets the following criteria:\\n- The text should be informative and engaging, targeting a general\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def llamacpp_inference(prompt, n_predict=128, temperature=0.9, top_p=0.95, stop=None):\n",
    "    url = \"http://localhost:8088/completion\"\n",
    "    \n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"n_predict\": n_predict,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"stop\": stop if stop else [],\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result['content']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Building a website can be done in 10 simple steps:\"\n",
    "result = llamacpp_inference(prompt, n_predict=200, temperature=0.8, stop=[\"\\n\\n\"])\n",
    "\n",
    "if result:\n",
    "    print(\"Generated text:\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "\n",
    "\n",
    "def call_chat_api(user_input):\n",
    "    url = \"http://34.64.46.1:8090/chat\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"user_input\": user_input,\n",
    "        \"index_id\": \"files\",\n",
    "        \"llm_text\": \"local\",\n",
    "        \"dense_top_k\": 4,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, stream=True)\n",
    "        if response.status_code == 200:\n",
    "                return response.iter_content(chunk_size=None, decode_unicode=True)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            return iter([])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any request-related errors\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return iter([])    \n",
    "\n",
    "\n",
    "def chat(chatbot_history):\n",
    "    print(\"history: \", chatbot_history)\n",
    "        \n",
    "    user_input = chatbot_history[-1][0] # idx-0 --> User input\n",
    "    streamer = call_chat_api(user_input)\n",
    "    for chunk in streamer:\n",
    "        chatbot_history[-1][1] += chunk\n",
    "        yield chatbot_history\n",
    "    return chatbot_history\n",
    "\n",
    "\n",
    "def chatbot_history_collection(input_query, chat_history):\n",
    "    print(input_query, chat_history)\n",
    "    if input_query is None or len(input_query) == 0:\n",
    "        input_query=\"\"\n",
    "\n",
    "    return \"\", chat_history + [[input_query, '']]\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    # Header with a professional title and subtitle\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        <h1 style=\"text-align: center; color: #3b3b3b;\">💬 FastRAG Chatbot</h1>\n",
    "        <h3 style=\"text-align: center; color: #666;\">Upload any PDF and ask anything</h3>\n",
    "        \"\"\",\n",
    "        elem_id=\"header\"\n",
    "    )\n",
    "\n",
    "\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot-display\")\n",
    "    input_text = gr.Textbox(\n",
    "                    placeholder=\"Type your message...\",\n",
    "                    show_label=False,\n",
    "                    lines=1,\n",
    "                    elem_id=\"user-input\"\n",
    "                )\n",
    "    with gr.Loader(\n",
    "        label=\"Parser is loading...\",\n",
    "        show_in_progress_button=False,\n",
    "        show_modal_on_error=True,\n",
    "        show_modal_on_completion=False,\n",
    "        show_loadingbar=True,\n",
    "        ):\n",
    "        get_conversation_history()\n",
    "    with gr.Row():\n",
    "        clear_submit_btn = gr.ClearButton(visible=True)\n",
    "        input_submit_btn= gr.Button(\"Submit\", visible=True)\n",
    "        stop_btn = gr.Button(\"Stop\", visible=True)\n",
    "\n",
    "    clear_submit_btn.add(\n",
    "        components=[chatbot, input_text]\n",
    "    )\n",
    "    \n",
    "    submit_event = input_text.submit(\n",
    "        fn = chatbot_history_collection,\n",
    "        inputs=[input_text, chatbot],\n",
    "        outputs=[input_text, chatbot],\n",
    "    ).then(\n",
    "        fn = chat,\n",
    "        inputs = [chatbot],\n",
    "        outputs=[chatbot]\n",
    "    )\n",
    "    \n",
    "    click_event = input_submit_btn.click(\n",
    "        fn = chatbot_history_collection,\n",
    "        inputs=[input_text, chatbot],\n",
    "        outputs=[input_text, chatbot],\n",
    "    ).then(\n",
    "        fn = chat,\n",
    "        inputs = [chatbot],\n",
    "        outputs=[chatbot]\n",
    "    )\n",
    "    \n",
    "    stop_btn.click(fn=None, inputs=None, outputs=None, cancels=[click_event, submit_event])\n",
    "\n",
    "    \n",
    "\n",
    "# Add custom CSS styling for a professional look\n",
    "demo.css = \"\"\"\n",
    "#chat-container {\n",
    "    max-width: 600px;\n",
    "    margin: 0 auto;\n",
    "}\n",
    "\n",
    "#chatbot-display {\n",
    "    border: 1px solid #dedede;\n",
    "    border-radius: 8px;\n",
    "    background-color: #f7f8fa;\n",
    "    padding: 20px;\n",
    "    color: #333333;\n",
    "    font-family: Arial, sans-serif;\n",
    "}\n",
    "\n",
    "#user-input {\n",
    "    border: 1px solid #aaaaaa;\n",
    "    padding: 10px;\n",
    "    border-radius: 8px;\n",
    "    width: 100%;\n",
    "}\n",
    "\n",
    "#send-button {\n",
    "    background-color: #0055a5;\n",
    "    color: #ffffff;\n",
    "    border-radius: 8px;\n",
    "    padding: 10px 20px;\n",
    "    border: none;\n",
    "    cursor: pointer;\n",
    "}\n",
    "\n",
    "#send-button:hover {\n",
    "    background-color: #004080;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def fetch_conversation_history():\n",
    "    # Simulate fetching conversation history\n",
    "    time.sleep(2)  # Simulating a delay\n",
    "    return [[\"User\", \"Hello\"], [\"AI\", \"Hi there!\"]]\n",
    "\n",
    "def load_chatbot():\n",
    "    history = fetch_conversation_history()\n",
    "    return gr.update(value=history, visible=True), gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# 💬 FastRAG Chatbot\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            chatbot = gr.Chatbot(visible=False)\n",
    "            input_text = gr.Textbox(\n",
    "                placeholder=\"Type your message...\",\n",
    "                show_label=False,\n",
    "                lines=1,\n",
    "                visible=False\n",
    "            )\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            load_button = gr.Button(\"Load Chatbot\")\n",
    "    \n",
    "    loading_indicator = gr.Markdown(\"Loading chatbot...\", visible=False)\n",
    "\n",
    "    def show_loading():\n",
    "        return gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "    load_button.click(\n",
    "        show_loading,\n",
    "        outputs=[loading_indicator, load_button]\n",
    "    ).then(\n",
    "        load_chatbot,\n",
    "        outputs=[chatbot, input_text, loading_indicator]\n",
    "    )\n",
    "\n",
    "    demo.load(\n",
    "        load_chatbot,\n",
    "        outputs=[chatbot, input_text, loading_indicator]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.Markdown(\n",
    "    \"\"\"\n",
    "    <style>\n",
    "        /* Header styling */\n",
    "        #header h1 {\n",
    "            font-size: 2.5em;\n",
    "            font-weight: bold;\n",
    "            color: #2c3e50; /* Darker shade for text */\n",
    "            text-align: center;\n",
    "            margin-bottom: 0.2em;\n",
    "        }\n",
    "        #header h3 {\n",
    "            font-size: 1.2em;\n",
    "            color: #7f8c8d; /* Soft gray for subtext */\n",
    "            text-align: center;\n",
    "            margin-top: 0.2em;\n",
    "            margin-bottom: 1.5em;\n",
    "        }\n",
    "\n",
    "        /* Container styling */\n",
    "        body {\n",
    "            font-family: 'Roboto', sans-serif;\n",
    "            background-color: #ecf0f1; /* Light background */\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "        }\n",
    "\n",
    "        /* Loading Indicator Styling */\n",
    "        #loading-indicator {\n",
    "            font-size: 1.2em;\n",
    "            font-weight: bold;\n",
    "            color: #3498db; /* Soft blue for loading text */\n",
    "            text-align: center;\n",
    "            margin-top: 1em;\n",
    "            margin-bottom: 1em;\n",
    "        }\n",
    "    </style>\n",
    "    \n",
    "    <div id=\"header\">\n",
    "        <h1>💬 FastRAG Chatbot</h1>\n",
    "        <h3>Upload any PDF and ask anything</h3>\n",
    "    </div>\n",
    "    \n",
    "    <div id=\"loading-indicator\">Loading chatbot...</div>\n",
    "    \"\"\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastrag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
