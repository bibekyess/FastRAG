{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def llamacpp_inference(prompt, n_predict=128, temperature=0.7, top_p=0.95, stop=None, stream=True):\n",
    "    url = \"http://localhost:8088/completion\"\n",
    "    \n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"n_predict\": n_predict,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"stop\": stop if stop else [],\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Handle streaming response\n",
    "    def handle_streaming():\n",
    "        with requests.post(url, headers=headers, json=payload, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            for line in response.iter_lines(decode_unicode=True):\n",
    "                if line:  # Filter out keep-alive lines\n",
    "                    try:\n",
    "                        # Remove \"data: \" prefix and parse JSON\n",
    "                        data = json.loads(line[6:])\n",
    "                        yield data.get(\"content\", \"\")  # Yield the \"content\" field\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(\"Failed to decode JSON:\", line)\n",
    "\n",
    "    # Handle non-streaming response\n",
    "    def handle_non_streaming():\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result.get('content', '')\n",
    "\n",
    "    if stream:\n",
    "        return handle_streaming()\n",
    "    else:\n",
    "        return handle_non_streaming()\n",
    "\n",
    "\n",
    "# Usage example with streaming enabled\n",
    "print(\"Streaming response:\")\n",
    "prompt = \"Explain the impact of artificial intelligence:\"\n",
    "# for chunk in llamacpp_inference(prompt, n_predict=100, temperature=0.8, stream=True):\n",
    "#     if chunk:\n",
    "#         print(chunk, end='')\n",
    "\n",
    "# Usage example with streaming disabled\n",
    "print(\"\\n\\nNon-streaming response:\")\n",
    "full_response = llamacpp_inference(prompt, n_predict=100, temperature=0.8, stream=False)\n",
    "print(full_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def llamacpp_inference(prompt, n_predict=128, temperature=0.9, top_p=0.95, stop=None):\n",
    "    url = \"http://localhost:8088/completion\"\n",
    "    \n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"n_predict\": n_predict,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"stop\": stop if stop else [],\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result['content']\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Building a website can be done in 10 simple steps:\"\n",
    "result = llamacpp_inference(prompt, n_predict=200, temperature=0.8, stop=[\"\\n\\n\"])\n",
    "\n",
    "if result:\n",
    "    print(\"Generated text:\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "\n",
    "\n",
    "def call_chat_api(user_input):\n",
    "    url = \"http://34.64.46.1:8090/chat\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"user_input\": user_input,\n",
    "        \"index_id\": \"files\",\n",
    "        \"llm_text\": \"local\",\n",
    "        \"dense_top_k\": 4,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, stream=True)\n",
    "        if response.status_code == 200:\n",
    "                return response.iter_content(chunk_size=None, decode_unicode=True)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            return iter([])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle any request-related errors\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return iter([])    \n",
    "\n",
    "\n",
    "def chat(chatbot_history):\n",
    "    print(\"history: \", chatbot_history)\n",
    "        \n",
    "    user_input = chatbot_history[-1][0] # idx-0 --> User input\n",
    "    streamer = call_chat_api(user_input)\n",
    "    for chunk in streamer:\n",
    "        chatbot_history[-1][1] += chunk\n",
    "        yield chatbot_history\n",
    "    return chatbot_history\n",
    "\n",
    "\n",
    "def chatbot_history_collection(input_query, chat_history):\n",
    "    print(input_query, chat_history)\n",
    "    if input_query is None or len(input_query) == 0:\n",
    "        input_query=\"\"\n",
    "\n",
    "    return \"\", chat_history + [[input_query, '']]\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    # Header with a professional title and subtitle\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        <h1 style=\"text-align: center; color: #3b3b3b;\">ðŸ’¬ FastRAG Chatbot</h1>\n",
    "        <h3 style=\"text-align: center; color: #666;\">Upload any PDF and ask anything</h3>\n",
    "        \"\"\",\n",
    "        elem_id=\"header\"\n",
    "    )\n",
    "\n",
    "\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot-display\")\n",
    "    input_text = gr.Textbox(\n",
    "                    placeholder=\"Type your message...\",\n",
    "                    show_label=False,\n",
    "                    lines=1,\n",
    "                    elem_id=\"user-input\"\n",
    "                )\n",
    "    \n",
    "    with gr.Row():\n",
    "        clear_submit_btn = gr.ClearButton(visible=True)\n",
    "        input_submit_btn= gr.Button(\"Submit\", visible=True)\n",
    "        stop_btn = gr.Button(\"Stop\", visible=True)\n",
    "\n",
    "    clear_submit_btn.add(\n",
    "        components=[chatbot, input_text]\n",
    "    )\n",
    "    \n",
    "    submit_event = input_text.submit(\n",
    "        fn = chatbot_history_collection,\n",
    "        inputs=[input_text, chatbot],\n",
    "        outputs=[input_text, chatbot],\n",
    "    ).then(\n",
    "        fn = chat,\n",
    "        inputs = [chatbot],\n",
    "        outputs=[chatbot]\n",
    "    )\n",
    "    \n",
    "    click_event = input_submit_btn.click(\n",
    "        fn = chatbot_history_collection,\n",
    "        inputs=[input_text, chatbot],\n",
    "        outputs=[input_text, chatbot],\n",
    "    ).then(\n",
    "        fn = chat,\n",
    "        inputs = [chatbot],\n",
    "        outputs=[chatbot]\n",
    "    )\n",
    "    \n",
    "    stop_btn.click(fn=None, inputs=None, outputs=None, cancels=[click_event, submit_event])\n",
    "\n",
    "    \n",
    "\n",
    "# Add custom CSS styling for a professional look\n",
    "demo.css = \"\"\"\n",
    "#chat-container {\n",
    "    max-width: 600px;\n",
    "    margin: 0 auto;\n",
    "}\n",
    "\n",
    "#chatbot-display {\n",
    "    border: 1px solid #dedede;\n",
    "    border-radius: 8px;\n",
    "    background-color: #f7f8fa;\n",
    "    padding: 20px;\n",
    "    color: #333333;\n",
    "    font-family: Arial, sans-serif;\n",
    "}\n",
    "\n",
    "#user-input {\n",
    "    border: 1px solid #aaaaaa;\n",
    "    padding: 10px;\n",
    "    border-radius: 8px;\n",
    "    width: 100%;\n",
    "}\n",
    "\n",
    "#send-button {\n",
    "    background-color: #0055a5;\n",
    "    color: #ffffff;\n",
    "    border-radius: 8px;\n",
    "    padding: 10px 20px;\n",
    "    border: none;\n",
    "    cursor: pointer;\n",
    "}\n",
    "\n",
    "#send-button:hover {\n",
    "    background-color: #004080;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastrag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
